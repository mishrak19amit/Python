{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting UPSC rank using linear regression of Pyspark\n",
    "#!pip install Bio\n",
    "#!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Predicting_upsc_rank\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsc_data=spark.read.csv(\"/home/moglix/Desktop/Amit/PGitHub/Python/UPSC_Result/CSM18_FQ_WEB_CELL_NEW.csv\",inferSchema=True,header=True)\n",
    "#upsc_data=upsc_data.drop(['ROLL_NO', 'NAME'])\n",
    "#Dropping roll_num and name because it's not a important feature\n",
    "columns_to_drop = ['ROLL_NO', 'NAME']\n",
    "upsc_data = upsc_data.drop(*columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+-------+----+\n",
      "|CATEGORY|PT_MARKS|MAIN_MARKS|F_TOTAL|RANK|\n",
      "+--------+--------+----------+-------+----+\n",
      "|      SC|     179|       942|   1121|   1|\n",
      "|      UR|     198|       882|   1080|   2|\n",
      "|      UR|     184|       893|   1077|   3|\n",
      "|      UR|     184|       887|   1071|   4|\n",
      "|      UR|     173|       895|   1068|   5|\n",
      "|      UR|     184|       883|   1067|   6|\n",
      "|      UR|     182|       885|   1067|   7|\n",
      "|      UR|     195|       871|   1066|   8|\n",
      "|      UR|     193|       871|   1064|   9|\n",
      "|      UR|     187|       877|   1064|  10|\n",
      "|      UR|     193|       870|   1063|  11|\n",
      "|      UR|     171|       891|   1062|  12|\n",
      "|      UR|     165|       897|   1062|  13|\n",
      "|      UR|     182|       879|   1061|  14|\n",
      "|      UR|     206|       854|   1060|  15|\n",
      "|     OBC|     176|       884|   1060|  16|\n",
      "|      UR|     193|       866|   1059|  17|\n",
      "|      UR|     180|       879|   1059|  18|\n",
      "|      UR|     171|       887|   1058|  19|\n",
      "|      UR|     193|       864|   1057|  20|\n",
      "+--------+--------+----------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#upsc_data=upsc_data.na.fill(\"UR\", \"CATEGORY\")\n",
    "# Filling null value for Category to best fit in feature model\n",
    "colname=[\"CATEGORY\"]\n",
    "upsc_data=upsc_data.na.fill(\"UR\",colname);\n",
    "upsc_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because Category is a String so need to reindex it as a double to fit in feature model\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer=StringIndexer(inputCol='CATEGORY', outputCol='CATEGORY_cat')\n",
    "indexed=indexer.fit(upsc_data).transform(upsc_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+-------+----+------------+\n",
      "|CATEGORY|PT_MARKS|MAIN_MARKS|F_TOTAL|RANK|CATEGORY_cat|\n",
      "+--------+--------+----------+-------+----+------------+\n",
      "|      SC|     179|       942|   1121|   1|         2.0|\n",
      "|      UR|     198|       882|   1080|   2|         0.0|\n",
      "|      UR|     184|       893|   1077|   3|         0.0|\n",
      "|      UR|     184|       887|   1071|   4|         0.0|\n",
      "|      UR|     173|       895|   1068|   5|         0.0|\n",
      "|      UR|     184|       883|   1067|   6|         0.0|\n",
      "|      UR|     182|       885|   1067|   7|         0.0|\n",
      "|      UR|     195|       871|   1066|   8|         0.0|\n",
      "|      UR|     193|       871|   1064|   9|         0.0|\n",
      "|      UR|     187|       877|   1064|  10|         0.0|\n",
      "|      UR|     193|       870|   1063|  11|         0.0|\n",
      "|      UR|     171|       891|   1062|  12|         0.0|\n",
      "|      UR|     165|       897|   1062|  13|         0.0|\n",
      "|      UR|     182|       879|   1061|  14|         0.0|\n",
      "|      UR|     206|       854|   1060|  15|         0.0|\n",
      "|     OBC|     176|       884|   1060|  16|         1.0|\n",
      "|      UR|     193|       866|   1059|  17|         0.0|\n",
      "|      UR|     180|       879|   1059|  18|         0.0|\n",
      "|      UR|     171|       887|   1058|  19|         0.0|\n",
      "|      UR|     193|       864|   1057|  20|         0.0|\n",
      "+--------+--------+----------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             feature|RANK|\n",
      "+--------------------+----+\n",
      "|[179.0,942.0,1121...|   1|\n",
      "|[198.0,882.0,1080...|   2|\n",
      "|[184.0,893.0,1077...|   3|\n",
      "|[184.0,887.0,1071...|   4|\n",
      "|[173.0,895.0,1068...|   5|\n",
      "|[184.0,883.0,1067...|   6|\n",
      "|[182.0,885.0,1067...|   7|\n",
      "|[195.0,871.0,1066...|   8|\n",
      "|[193.0,871.0,1064...|   9|\n",
      "|[187.0,877.0,1064...|  10|\n",
      "|[193.0,870.0,1063...|  11|\n",
      "|[171.0,891.0,1062...|  12|\n",
      "|[165.0,897.0,1062...|  13|\n",
      "|[182.0,879.0,1061...|  14|\n",
      "|[206.0,854.0,1060...|  15|\n",
      "|[176.0,884.0,1060...|  16|\n",
      "|[193.0,866.0,1059...|  17|\n",
      "|[180.0,879.0,1059...|  18|\n",
      "|[171.0,887.0,1058...|  19|\n",
      "|[193.0,864.0,1057...|  20|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training model based on Main_marks, PT_marks, and Category, and total marks considering these a a feature\n",
    "# Rank is the output \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "\n",
    "assembler=VectorAssembler(inputCols=['PT_MARKS', \n",
    " 'MAIN_MARKS', \n",
    " 'F_TOTAL', \n",
    " 'CATEGORY_cat'], outputCol='feature')\n",
    "output=assembler.transform(indexed)\n",
    "output.select('feature','RANK').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data model in 80% and 20% ratio\n",
    "final_data=output.select('feature','RANK')\n",
    "train_data, test_data=final_data.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              RANK|\n",
      "+-------+------------------+\n",
      "|  count|               683|\n",
      "|   mean| 383.6281112737921|\n",
      "| stddev|220.36375399922917|\n",
      "|    min|                 1|\n",
      "|    max|               759|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              RANK|\n",
      "+-------+------------------+\n",
      "|  count|                76|\n",
      "|   mean|347.39473684210526|\n",
      "| stddev|207.50030226145813|\n",
      "|    min|                19|\n",
      "|    max|               753|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.8844123851990178\n"
     ]
    }
   ],
   "source": [
    "# Using Linear Regression from Spark ML Library\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "ship_lr=LinearRegression(featuresCol='feature', labelCol='RANK')\n",
    "trainded_ship_model=ship_lr.fit(train_data)\n",
    "ship_result=trainded_ship_model.evaluate(train_data)\n",
    "\n",
    "# Getting accuracy score for trained model\n",
    "print('Score :',ship_result.r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(feature=DenseVector([135.0, 818.0, 953.0, 1.0])),\n",
       " Row(feature=DenseVector([140.0, 851.0, 991.0, 0.0])),\n",
       " Row(feature=DenseVector([143.0, 638.0, 781.0, 1.0])),\n",
       " Row(feature=DenseVector([143.0, 806.0, 949.0, 1.0])),\n",
       " Row(feature=DenseVector([143.0, 850.0, 993.0, 0.0])),\n",
       " Row(feature=DenseVector([143.0, 868.0, 1011.0, 0.0])),\n",
       " Row(feature=DenseVector([145.0, 793.0, 938.0, 1.0])),\n",
       " Row(feature=DenseVector([146.0, 844.0, 990.0, 1.0])),\n",
       " Row(feature=DenseVector([151.0, 792.0, 943.0, 1.0])),\n",
       " Row(feature=DenseVector([151.0, 815.0, 966.0, 2.0]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data=test_data.select('feature')\n",
    "unlabeled_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the test data and pre\n",
    "predictions=trainded_ship_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             feature|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[135.0,818.0,953....| 472.1268552907768|\n",
      "|[140.0,851.0,991....| 271.6768740369712|\n",
      "|[143.0,638.0,781....|1137.4608072326082|\n",
      "|[143.0,806.0,949....|488.55883722811905|\n",
      "|[143.0,850.0,993....|264.32007622171477|\n",
      "|[143.0,868.0,1011...|194.79486514980454|\n",
      "|[145.0,793.0,938....| 531.2919499746267|\n",
      "|[146.0,844.0,990....|330.56408209039137|\n",
      "|[151.0,792.0,943....| 512.7158426178953|\n",
      "|[151.0,815.0,966....| 478.1663179677471|\n",
      "|[154.0,773.0,927....|  629.172500927396|\n",
      "|[154.0,850.0,1004...| 223.1826079029761|\n",
      "|[155.0,849.0,1004...|223.30534978203514|\n",
      "|[157.0,843.0,1000...|239.00088044502172|\n",
      "|[160.0,755.0,915....| 676.2590929163571|\n",
      "|[160.0,779.0,939....| 529.2705664342966|\n",
      "|[160.0,783.0,943....| 513.8205195294277|\n",
      "|[160.0,788.0,948....|440.21971584549374|\n",
      "|[160.0,819.0,979....|374.77009738560855|\n",
      "|[160.0,861.0,1021...| 158.2563598316383|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
